---
title: "Data624 Project2"
author: "Nnaemezue Obi-Eyisi"
date: "April 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```


Import Libraries
```{r}
library(xlsx)
library(tidyr)
library(earth)
library(AppliedPredictiveModeling)
library(caret)
library(ggplot2)
library(mice)
library(missForest)
library(randomForest)
library(corrplot)
library(xgboost)
library(DiagrammeR)
#install.packages("stringi")
```


#Problem statement

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

```{r}

Student_data <-  readxl::read_excel("C:/Users/Mezu/Documents/Data624/StudentData.xlsx")
head(Student_data)

summary(Student_data)

```

#Missing Values Analysis
All predictors have missing values except  Brand Code,Pressure Vacuum, Air Pressurer

Since our response has missing values let's filter it out
```{r}
Student_data_df <- subset(Student_data,is.na(`PH`) == FALSE)
summary(Student_data_df)
```

Make Brand code a factor
```{r}
Student_data_df$`Brand Code` <- factor( Student_data_df$`Brand Code`)
str(Student_data_df)

```


Distribution of Brand code
```{r}
barplot(prop.table(table(Student_data$`Brand Code`)))

```
Impute Missing values using Randomforest
```{r}

myvars <- names(Student_data_df) %in% c("PH")
Student_data.imp <- Student_data_df[!myvars]
#make Brand code a factor


student_df_missForest <-rfImpute(PH ~ ., Student_data_df)
#create new numeric labels for brand code
student_df_missForest$`BrandCode_num` <- as.numeric(factor( student_df_missForest$`Brand Code`))

summary(student_df_missForest2)

```

Let's check the correlation of the predictors
```{r}
myvars <- names(student_df_missForest) %in% c("Brand Code")
student_df_missForest2<- student_df_missForest[, !myvars]
student_df_missForest2_cor <-cor(student_df_missForest2)
corrplot(student_df_missForest2_cor)
```
```{r}
## 75% of the sample size
smp_size <- floor(0.75 * nrow(student_df_missForest2))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(student_df_missForest2)), size = smp_size)

train <- student_df_missForest2[train_ind, ]
test <-student_df_missForest2[-train_ind, ]
TrainXtrans <- train[, !names(train) %in% "PH"]
TrainY <- train[,  "PH"]

TestXtrans <- test[, !names(train) %in% "PH"]
TestY <- test[,  "PH"]
ctrl <- trainControl(method = "cv", number = 10)
```

#Extreme Gradient Boost Modelling (XGBoost)


EXtreme Gradient Boosting, was developed by Tianqi Chen and now is part of a wider collection of open-source libraries developed by the Distributed Machine Learning Community (DMLC). XGBoost is a scalable and accurate
implementation of gradient boosting machines and it has proven to push the limits of computing
power for boosted trees algorithms as it was built and developed for the sole purpose of model
performance and computational speed. 

It is based on Gradient Boosting alogorithm is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.

This model was choosen for this project due to it's unique qualities.
```{r}
tuneGrid <- expand.grid(.nrounds=c(10,20,50),      # boosting iterations (trees)
                        .max_depth=c(6, 10, 20),     # max tree depth
                        .eta=c(0.3, 0.01, 0.1),      # learning rate
                        .gamma=c(0, 5),              # minimum loss reduction
                        .colsample_bytree=c(1, 0.5), # subsample ratio of columns
                        .min_child_weight=c(1, 5),   # minimum sum of instance weight
                        .subsample=c(0.1, 0.5))      # subsample ratio of rows

# total models = 3*3*3*2*2*2*2*10 = 4320
set.seed(1)
bst <- train(x = TrainXtrans,
             y = TrainY,
             method = 'xgbTree',
             tuneGrid = tuneGrid,
             trControl = trainControl(method='cv'))
```
```{r}
bst$bestTune
```

```{r}
bst$finalModel
```

```{r}
#xgb.plot.tree(model=bst$finalModel, trees=2)
```

```{r}
plot(varImp(bst))
```
```{r}
xgboostTunePred <- predict(bst, newdata = TestXtrans)
postResample(pred =xgboostTunePred, obs = TestY) 
```
We clearly see that the most important predictors are Usage cont, Mini Flow, Oxygen Filter etc. We also got an RMSE of 0.1023


MARS model
MARS model Multivariate Adaptive Regression Splines (MARSplines) which is an implementation of techniques popularized by Friedman (1991) for solving regression-type problems. It's main purpose is to predict the values of a continuous dependent or outcome variable from a set of independent or predictor variables. The reason I chose the MARSplines is because it is a nonparametric regression procedure that makes no assumption about the underlying functional relationship between the dependent and independent variables. Since in this case it was not clear if there was linear relationship or not. It is great even in situations where the relationship between the predictors and the dependent variables is non-monotone and difficult to approximate with parametric models

```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38) 
set.seed(100)


MarsModel <- train(x = TrainXtrans,
             y = TrainY,
              method = "earth",
             tuneGrid = marsGrid,
             trControl = trainControl(method='cv'))
```

```{r}
MarsModel$bestTune

plot(varImp(MarsModel))
```
```{r}
MarsModelTunePred <- predict(MarsModel, newdata = TestXtrans)
postResample(pred =MarsModelTunePred, obs = TestY) 
```

We can see that the RSME is lower than the Xgboost model.
The most important predictors seem to be the MnfFlow, Brand_code, Airpressure